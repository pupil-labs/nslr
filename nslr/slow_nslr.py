# Copyleft 2017 Jami Pekkanen <jami.pekkanen@gmail.com>.
# Released under AGPL-3.0, see LICENSE.

# Pure python implementation of NSLR. A lot slower than
# the C++ implementation, but it's insane how difficult building
# and distributing a little piece of software can be in this age,
# especially for certain proprietary operating systems.

import numpy as np
import scipy.interpolate

# Data structure for a segmentation hypothesis
class Hypothesis: 
    def __init__(self, D, j, parent, lik, b):
        # Initialize parent-dependent variables
        self.j = j; self.parent = parent; self.lik = lik; self.b = b;
        # Initialize accumulators to zero
        self.a = None; self.n = 0
        self.t = 0.0; self.St = 0.0; self.Stt = 0.0; self.S = 0
        zeros = np.zeros
        self.Sx = zeros(D); self.Sxx = zeros(D); self.Stx = zeros(D)

def nslr_segments(ts, xs, noise_stds, split_lik):
    ts = ts.reshape(-1, 1)
    D = len(noise_stds) # Number of dimensions in the dependent variable
    # Initialize the hypothesis set with a "root" hypothesis
    hypotheses = [ Hypothesis(D, j=0, parent=None, lik=0.0, b=None) ]
    prev_t = ts[0]
    zeros = np.zeros; log = np.log; sqrt = np.sqrt; pi = np.pi
    for i, (t, x) in enumerate(zip(ts, xs)): # Iterate over all samples
        dt = t - prev_t; prev_t = t
        for h in hypotheses:
            # Update the accumulator variables
            h.n += 1; h.t += dt; h.St += h.t; h.Stt += h.t**2
            h.Sx += x; h.Sxx += x**2; h.Stx += h.t*x
            
            if h.parent is None: # Calculate the intercept for the root hypothesis
                if (h.St**2 - h.n*h.Stt) > 0:
                    h.b = (h.St*h.Stx - h.Stt*h.Sx)/(h.St**2 - h.n*h.Stt)
                else:
                    h.b = h.Sx/h.n
            
            # Calculate the slope and the residual sums of squares
            if h.Stt > 0.0: 
                h.a = (h.Stx - h.b*h.St)/h.Stt
                S = h.a**2*h.Stt + 2*h.a*h.b*h.St - 2*h.a*h.Stx + h.n*h.b**2 - 2*h.b*h.Sx + h.Sxx
            else:
                S = zeros(D)
            
            # Update the hypothesis' fitness function
            h.lik += sum(log(1/(sqrt(2*pi)*noise_stds))) + sum((h.S - S)/(2*noise_stds**2))
            h.S = S
        
        if i == 0: continue # First hypothesis needs at least two samples
        
        winner = max(hypotheses, key=lambda h: h.lik) # Get the current most likely hypothesis
        # Create a new hypothesis starting from the winner's endpoint.
        winner_end = winner.t*winner.a + winner.b
        new_lik = winner.lik + split_lik(dt) # Add the split likelihood
        new_h = Hypothesis(D, j=i, parent=winner, lik=new_lik, b=winner_end)
        
        # Prune hypotheses that have lower fitness than the newly formed hypothesis
        hypotheses = [h for h in hypotheses if h.lik > new_lik or h is winner]
        hypotheses.append(new_h)

    # Find the split points J
    h = max(hypotheses, key=lambda h: h.lik)
    splits = [len(ts)]
    while h is not None:
        splits.append(h.j); h = h.parent
    return splits[::-1]

# Produces the matrix coefficients for solving
# the least squares continuous segmented regression
def segment_coefficients(ts, xs, J):
    ts = ts.reshape(-1, 1)
    Smw0 = 0.0; Smw0 = 0.0; Sxw0 = 0.0; Sww0 = 0.0; Smm0 = 0.0
    for k in range(len(J) - 1):
        span = slice(J[k], J[k+1])
        t = ts[span]; x = xs[span]
        dur = (t[-1] - t[0])
        if dur == 0:
            dur = 1.0
        w = (t - t[0])/dur
        m = 1 - w
        
        Smw1 = (m*w).sum()
        Smm1 = (m*m).sum()
        Sxm1 = (x*m).sum(axis=0)

        p0 = Smw0
        p1 = Smm1 + Sww0
        p2 = Smw1
        y = Sxm1 + Sxw0
        yield p0, p1, p2, y

        Smw0 = Smw1
        Smm0 = Smm1
        Sxm0 = Sxm1
        Sww0 = (w*w).sum()
        Sxw0 = (x*w).sum(axis=0)
    p0 = Smw0
    p1 = 0.0 + Sww0
    p2 = 0.0
    y = 0.0 + Sxw0
    yield p0, p1, p2, y

        
# Uses the tridiagonal matrix algorithm to solve the
# system generated by segment_coefficients. Returns
# the endpoints of the linear segments at times ts[sis]
def segmented_linear_fit(ts, xs, J):
    bgs = [(0.0, 0.0)]
    for p0, p1, p2, y in segment_coefficients(ts, xs, J):
        b, g = bgs[-1]
        denom = p0*g + p1
        bgs.append(( (y - p0*b)/denom, -p2/denom ))

    endpoint = 0.0
    endpoints = []
    for b, g in reversed(bgs[1:]):
        endpoint = g*endpoint + b
        endpoints.append(endpoint)
    return endpoints[::-1]

class Segment(object):
    def __init__(self, i, t, x):
        self.i = i
        self.t = t
        self.x = x

class Segmentation(object):
    def __init__(self, t, x, idx):
        self.t = t
        self.x = x
        self.segments = []
        for i in range(len(t)-1):
            s = Segment(
                    (idx[i], idx[i+1]),
                    (t[i], t[i+1]),
                    (x[i], x[i+1]))
            self.segments.append(s)
        self.interp = scipy.interpolate.interp1d(self.t, self.x, fill_value='extrapolate', axis=0)

    def __call__(self, nt):
        return self.interp(nt)

# Runs the whole NSLR. Returns times at segment endpoints,
# estimated dependent variable values at endpoints and
# the segment endpoint indices.
def nslr(ts, xs, noise_stds, split_likelihood):
    J = nslr_segments(ts, xs, noise_stds, split_likelihood)
    endpoints = segmented_linear_fit(ts, xs, J)
    tidx = J[::]
    tidx[-1] -= 1
    return Segmentation(ts[tidx], endpoints, J)

def gaze_split(noise_std,
        saccade_amplitude=3.0,
        slow_phase_duration=0.3,
        slow_phase_speed=5.0):
    log = np.log; exp = np.exp
    logit_pinc = (
        0.5*(1.0/noise_std) +
        0.5*log(saccade_amplitude*2) +
        -1.0*log(slow_phase_duration*2) +
        0.1*log(slow_phase_speed*2) +
        -3.0)
    def lik(dt):
        lp = logit_pinc + -1.0*log(1/dt)
        return log(1/(1 + exp(-lp)))
    return lik

def fit_gaze(ts, xs, structural_error=0.1,
        optimize_noise=True,
        split_likelihood=gaze_split):
    std = np.std; mean = np.mean
    structural_error = np.ones(2)*structural_error
    if not optimize_noise:
        return nslr(ts, xs, structural_error, gaze_split(mean(structural_error)))

    nl = std(xs, axis=0)
    seen = set([tuple(nl)])
    while True:
        nl += structural_error
        fit = nslr(ts, xs, nl, gaze_split(mean(nl)))
        error = fit(ts) - xs
        nl = std(error, axis=0)
        if tuple(nl) in seen:
            return fit
        seen.add(tuple(nl))
